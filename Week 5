COST FUNCTION AND BACKPROPOGATION
L = total number of layers in network
sl = number of units (not counting bias unit) in layer l
K classes... K output units
h(x)i = ith output of k dim vector
J(Θ)=−1/m*summ(sumk[yk^i * log((hΘ(x^i))subk)+(1−yk^i)*log(1−(hΘ(x^i))subk)]
                                      +λ/2m * sumltoL-1(sumitosj(sumjtosj+1(Θ(j,i)^l)^2
In the regularization part we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term.
the double sum simply adds up the logistic regression costs calculated for each cell in the output layer
the triple sum simply adds up the squares of all the individual Θs in the entire network.
the i in the triple sum does not refer to training example i
forward propoation:
a=x
z=theta*a
a2=g(z2)
z3=theta2*a2
.....(given one training example)
gradient computation:back propogation
back propogation is a neural network term for minimizing our cost funtion, just like what we were doing with gradient descent in logistiv and linear regression
delj^4=aj^4-yj   hypothesis - output... obvious error calc
del^3=theta^3trans*del^4.*(a^3.*(1-a^3))
del^2=theta^2trans*del^3.*(a^2.*(1-a^2))
partilJ(theta) = aj^l*deli^(l+1) ignoring lambda
for full training set...
DELij^l = 0 (for all l,i,j) .. these serve as accumulators
For i=1 to m,
  set a^1=x^i
  perform forward prop to compute a^l for l 1 to L
  Use y^i to compute del^L = a^L-y^i
  compute del ^(L_1), del^(L-2),...,del^2
  DELij^l := DELij^l + aj^l * deli^(l+1)
Dij^l := 1/m * DELij^l + lambda*thetaij^l (no lambda when j=0) (this is your the partial)
Summary...
1.  a^1 := x^t
2. FP a^l
3. del^L = a^L - y^t
4. all dels
5. DEL, which we then use to update D accumulator matrix, eventually computering our partial derivative

BACKPROPOGATION IN PRACTICE
Unrolling Parameters
You need to unroll parameters from matrices to vectors to use advanced optimization techniques
thetavec = [theta1(:);theta2(:);theta3(:)];
Theta1 = reshape(thetavec(1:110),10,11);
  Learning Algorithm
  Have initial parameters theta^1, theta^2, theta^3
  Unroll to het initialTheta to pass to
  fminunc(@costFunction, initialTheta, options)
    function [jval, gradientVec] = costfunction (thetavec)
        From thetavec get theta^1, etc
        Use forward prop/back prop to compute D^1, D^2, D^3, and J(theta)
        Unroll D^1, D^2, D63 to get gradientVec
Gradient Checking
numerical estimate: gradapprox = (J(thet+epsilon)-J(theta-epsilon))/(2*epsilon)
  for i=1:n,
    thetplus = theta;
    thetaplus(i) = thetplus(i) +epsilon;
    thetaminus=theta;
    thetaminus(i) = thetaminus(i) - epsilon;
    gradapprox(i) = (J(thetaplus) - J(thetaminus))/(2*epsilon);
  end;
check that gradapprox~=DVec (from backprop)
1. implement backprop to compute Dvec
2. implement numerical gradient check to compute gradapprox
3. check against each other
4. turn off gradient checking before training the classifier
Random Initialization
itialize each theta to a random value from negative epsilon to positive epsilon 
  theta1 - rand(10,11) * (2*init_epsilon) - init_epsilon;
Putting it All Together
Training a neural network
0. Pick an architecture (reasonable default is one hidden layer. if you use more than one, keep he same number of hidden units in each layer)
1. randomly initiale weights
2. Implement forward prop to get htheta(x^i)
3. implement code to get the cost function
4. implement backprop to compute partial derivatives
    for i=1:m
      permorm forward prop and back prop using example (x^i,y^i)
      (get activations a^l) and delta terms del^l for l(2:L))
      Delta^l := Delta^l + del^(l^1) * a^l)trans
5. Use gradient checking to compare partialJ computed using back prop vs numerical estimate of gradient of J(theta)disable gradient chekcing
6. Use gradient descent or advanced optimization method with backpropogation to try to minimize J(theta) as a function of parameters... J(theta) is susceptible to local optima, but thi s isn't a huge problem in practice

  
 
  
APPLICATION OF NEURAL NETWORKS
