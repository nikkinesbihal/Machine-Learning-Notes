CLUSTERING (Unsupervised)
data center computing cluster organization, segmentation
no y labels
K Means Algorithm 
    cluster assignment: assign to cluster centroid depending on distance
    cluster moving: move to mean location of associated cluster 
    cluster assignment 2: reassign based on distance
    cluster moving 2: move to new mean
    cluster assignment and cluster moving one more time
    input: K, number of clusters; training set {x^1, ..., x^m}
    algorithm randomly initializes K cluster centroids, mu1,...,muk
      repeat{
          for i=m
            c^1 := index (from 1 to K) of cluster centroid closest to x^i  (find value of k that minimized distance)
          for k=1 to K
            muk:=average of points assigned to cluster (cluster centroid notation)
Optimization objective
 mc^i = cluster centroid of cluster to whicj example x^i has been assigned
J(c^1,...,c^m,mu1,...,muk) = 1/m * summ ||x^i - muc^i||^2     (minimize the cost function, or distortioon)
first loop is minimizing J(...) with c holding mu fixed.  second loop is minizing J(...) with mu, holding c fixed
Random Initialization
K<m, randomly pick K training examples, set mu1,...,muk equal to these K examples aka pick k distinct random integers i1...1k from {1...m} set mu1=x^(i1), mu2=x^(i2),...,muk=x^(ik)
can end up at local optima...try multiple random initializations and try multiple k means  
      for i=1 to (50-1000) {
          Randomly initialize K-means
          Run Kmean. Get c^1,...,c^m,mu1,...,muK
          Compute cost function (distortion)
              J(c^1,...,c^m,mu1,...,muK)
              }
      Pick clustering that gave lowest cost
Choosing number of clusters
Elbow method:when the rate of decrease of the cost function slows down
You can also evaluate kmeans based on a metric for how well it performs for that later purpose

DIMENSIONALITY REDUCTION
MOTIVATION
Data Compression
Reduce 2d to 1d, z is new feature for location of point
Suppose we apply dimensionality reduction to a dataset of m examples {x^1,...x^m} where x^i is within R^n. As a result of this, we will get out a lower dimensional dataset {z^1,...,z^m} of m examples where z^i is within R^k for some value k and k<=n 
Visualization
z's are summaries of other dimensions, look for pattern
PRINCIPAL COMPONENT ANALYSIS 
projection error - how far the 2d points are from your 1d flattened line
reduce from n dimensions to k dimension - find k vectors u^1, u^2,...,u^k onto which to project the data so as to miniize the projection error
preprocessing: featurescaling/mean normalization): muj = 1/m * summ xj^i ; replace each xj^i with xj-muj ; if different features on different scales (size of house, number of bedrooms), scale featers to have similar
compute "covariance matrix" : Sigma = 1/m sumn (x^i)*(x^i)trans : Sigma is a nxn matrix
compute eigenvenctors of matrix Sigma: [U,S,V] = svd(Sigma);  U matrix will also be nxn and the columns will be u1, u2 vectors that we need for the projections, take first k vectors
z = [Ureduce]transx 
vectorized: Sigma=(1/m) * Xtrans*X
Ureduce=U(:,1:k);
z=Ureduce'*x;

APPLYING PCA
reconstructing from compressed representation: xapprox = Ureduce*z
k is the number of principle components that we retain, tries to minimize average squared projection error
to choose k: (long way, without svd)
try pca w/ k=1
compute Ureduce, z^1, z^2,...,z^m,xapprox^1,...,xapprox^m
check if (summ ||x^i-xapprox^i||^2)/(summ||x^i||^2     <= .01 "99% of variance is retained"
BUT BUT For given k, that variance retained fraction is just (sumk(Sii))/sumn(Sii)) >= .99, vary k to find the smallest satisfying k
can speed up learning algorithm, (x^1,y^1),...,(x^m,y^m), usually supervised how to:
    extract inputs: unlabeled dataset x^1...x^m PCA to z1...z^m
    new training set: (z^1,y^1),...,(z^m,y^m)  feed this to algorithm and learn h
    PCA is only applied to training set, can apply mapping you find to cv and test set
 bad use of PCA : To prevent overfitting.  just use regularization instead
 Design of ML system:
    - Get training set {(x^1,y^1),...,(x^m,y^m)}
    - Run PCA to reduce x^i in dimension to get z^i
    - Train logistic regression on {(z^1,y^1),...,(z^m,y^m)}
    - Test on test set: Map xtest^i to ztest^i . Run htheta(z) on {(ztest^1, ytest^1,...,(ztest^,,ytest^m)}
 Before implementing PCA, first try tunning with original x data. If it doesn't work, implement PCA and consider using z^i
